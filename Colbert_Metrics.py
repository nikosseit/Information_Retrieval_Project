# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HzNAL24E393zJnZdCqfZFGp44ZHH_9TP
"""

!git -C ColBERT/ pull || git clone https://github.com/stanford-futuredata/ColBERT.git
import sys; sys.path.insert(0, 'ColBERT/')

try: # When on google Colab, let's install all dependencies with pip.
    import google.colab
    !pip install -U pip
    !pip install -e ColBERT/['faiss-gpu','torch']
except Exception:
  import sys; sys.path.insert(0, 'ColBERT/')
  try:
    from colbert import Indexer, Searcher
  except Exception:
    print("If you're running outside Colab, please make sure you install ColBERT in conda following the instructions in our README. You can also install (as above) with pip but it may install slower or less stable faiss or torch dependencies. Conda is recommended.")
    assert False

import colbert
from colbert import Indexer, Searcher
from colbert.infra import Run, RunConfig, ColBERTConfig
from colbert.data import Queries, Collection

import pandas as pd
import os
import matplotlib.pyplot as plt

text_list = [] #Listes gia th dhmiourgia twn katalhllwn dataframes
doc_list = []
query_id = []
query_text = []

for doc_id in range(1,1240): #Gia kathe doc_id se ayto to range anoixe ta arxeia me tetoio doc id efoson yparxoyn
  file_path = f'{doc_id:05}'+'.txt'

  exists = os.path.exists(file_path) #True an to arxeio yparxei
  if exists:
    f = open(file_path,'r')

    text = f.read() #Diavase to arxeio
    result = " ".join(line.strip() for line in text.splitlines()) #Enwse tis polles grammes toy arxeioy se mia
    doc_list.append(doc_id) #Prosthese to doc_id sth lista twn id
    text_list.append(result) #Prosthese to keimeno toy arxeioy sth lista arxeiwn
    f.close()

doc_data = { #Ftiaxe ena dict to opoio exei doc_id ta stoixeia ths listas id kai text ta stoixeia keimenoy gia kathe doc_id
  "doc_id": doc_list,
  "text": text_list
}

doc_df = pd.DataFrame(doc_data) #Dhmioyrghse to dataframe

collection = [doc_df.loc[x]['text'] for x in range(len(doc_df))] #Ftiaxe to collection gia to Colbert mesw toy dataframe

f = open('Queries_20.txt', 'r') #Idia diadikasia me th dhmiourgia toy collection gia ta queries

text = f.readlines()

count = 1
for line in text:
  query_id.append(count)
  count += 1
  query_text.append(line)

f.close()

q_data = {
  "query_id": query_id,
  "text": query_text
}

q_df = pd.DataFrame(q_data)

queries = [q_df.loc[x]['text'] for x in range(len(q_df))]

f'Loaded {len(queries)} queries and {len(collection):,} passages' #Emfanise posa queries kai keimena fortothikan

print(collection[2]) #Paradeigma enos arxeioy kai enos query
print()
print(queries[13])
print()

nbits = 2   #Dhmiourgia index name gia ton index ths sylloghs, arithmo bit gia thn kwdikopoihsh kai maximum arithmos tokens gia kathe arxeio
doc_maxlen = 300

index_name = f'IR-2024.{nbits}bits'

checkpoint = 'colbert-ir/colbertv2.0'

with Run().context(RunConfig(nranks=1, experiment='notebook')): #Vasei parametrwn kane dhmiourghse to configuration toy colbert
    config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits, kmeans_niters=4)


    indexer = Indexer(checkpoint=checkpoint, config=config) #Vasei toy config dhmioyrghse ton indexer
    indexer.index(name=index_name, collection=collection[:], overwrite=True) #Kane index olh thn syllogh

with Run().context(RunConfig(experiment='notebook')): #Dhmioyrghse ton searcher vasei toy index kai ths sylloghs
    searcher = Searcher(index=index_name, collection=collection)

relevant_id = [] #Paromoia diadikasia gia to arxeio relevant opws me ta arxeia docs, queries gia th dhmioyrgia enos dataframe poy tha xrhsimeysei ston ypologismo metrikwn
relevant_text = []
f = open('Relevant_20.txt', 'r')

text = f.readlines()

count = 1
for line in text:
  relevant_id.append(count)
  count += 1
  relevant_text.append(line.split()) #Split gia na lavei kathe stoixeio(dhladh kathe id) poy yparxei se kathe grammh toy relevant ws stoixeia listas kai oxi ws ena synoliko string

f.close()

relevant_data = {
  "relevant_id": relevant_id,
  "text": relevant_text
}

relevant_df = pd.DataFrame(relevant_data)

Average_Precision = [] #Listes gia tis metrikes
Reciprocal_Rank = []
k = 5
data =	{ #Ftiaxe ena dataframe gia to recall-precision diagramma opoy values oi times precision kai index oi times sta pososta gia to recall gia ola ta queries synolika
    "Precision": [0,0,0,0,0,0,0,0,0,0,0]
  }
Recall_Precision_df = pd.DataFrame(data, index = ["0%","10%","20%","30%","40%","50%","60%","70%","80%","90%","100%"])

num_queries = len(queries)
for query_id in range(num_queries): #Gia ola ta queries entos toy range
  rdoc_list = [] #Lista gia ta id tis apanthseis gia ena query
  query = queries[query_id]
  print(f"#> {query}") #Emfanise to query

  query_rp_df = pd.DataFrame(data, index = ["0%","10%","20%","30%","40%","50%","60%","70%","80%","90%","100%"]) #Dataframe gia to ekastote query ta apotelesmata toy opoioy tha prostethoyn sto geniko recall-precision dataframe
  num_rel = len(relevant_df.loc[query_id]['text']) #Orise th metavlhth num_rel ws to plithos twn sxetikwn keimenwn gia to ekastote query

  results = searcher.search(query, k) #Vres ta k docs ws results gia to query mesw toy searcher sth syllogh

  for passage_id, passage_rank, passage_score in zip(*results): #Gia kathe doc sto result vale to id sth lista me tis apanthseis kai emfanise to
      id = doc_df.loc[passage_id]['doc_id']
      rdoc_list.append(id)
      print(f"\t [{passage_rank}] \t\t {id} \t\t{passage_score:.1f} \t\t {searcher.collection[passage_id]}")

  count = 0 #Arithmos sxetikwn poy vriskontai stis apanthseis
  precision = [] #Lista gia to precision se kathe thesh poy yparxei sxetiko doc
  first = True #Metavlhth gia to prwto sxetiko doc gia ton ypologismo toy MRR
  for id in rdoc_list: #Gia kathe doc sth lista apanthsewn elegxe to me kathe doc sth lista sxetikwn
    for relevant in relevant_df.loc[query_id]['text']:
      if int(id) == int(relevant): #An to doc einai sxetiko me to query ayxhse to count
        count += 1
        position = rdoc_list.index(id) + 1 #Vres to position toy doc stis apanthseis (+1 kathos python xekina apo to 0)
        p = count/position #Ypologise to precision sth thesh ayth vasei twn sxetikwn (p@k)
        precision.append(p) #Prosthese to sth lista precision

        if first: #An to first true tote to id einai to prwto sxetiko stis apanthseis ypologise to rr kai prosthese to sth lista enw kane to first false
          rr = 1/position
          Reciprocal_Rank.append(rr)
          first = False

        recall = (count/num_rel)*100 #Vres to pososto anaklhshs analoga me to posa sxetika exoyn vrethei kathe fora

        if (0 <= recall) & (float(query_rp_df.loc["0%"]) == 0): #Gia kathe pososto mikrotero toy recall poy den exei oristei hdh orise thn timh toy ws to precision epi %
          query_rp_df.loc["0%"] = p*100
        if (10 <= recall) & (float(query_rp_df.loc["10%"]) == 0):
          query_rp_df.loc["10%"] = p*100
        if (20 <= recall) & (float(query_rp_df.loc["20%"]) == 0):
          query_rp_df.loc["20%"] = p*100
        if (30 <= recall) & (float(query_rp_df.loc["30%"]) == 0):
          query_rp_df.loc["30%"] = p*100
        if (40 <= recall) & (float(query_rp_df.loc["40%"]) == 0):
          query_rp_df.loc["40%"] = p*100
        if (50 <= recall) & (float(query_rp_df.loc["50%"]) == 0):
          query_rp_df.loc["50%"] = p*100
        if (60 <= recall) & (float(query_rp_df.loc["60%"]) == 0):
          query_rp_df.loc["60%"] = p*100
        if (70 <= recall) & (float(query_rp_df.loc["70%"]) == 0):
          query_rp_df.loc["70%"] = p*100
        if (80 <= recall) & (float(query_rp_df.loc["80%"]) == 0):
          query_rp_df.loc["80%"] = p*100
        if (90 <= recall) & (float(query_rp_df.loc["90%"]) == 0):
          query_rp_df.loc["90%"] = p*100
        if (100 == recall) & (float(query_rp_df.loc["100%"]) == 0):
          query_rp_df.loc["100%"] = p*100

  Recall_Precision_df = Recall_Precision_df.add(query_rp_df) #Prosthese ta apotelesmata gia recall-precision toy query sto synoliko dataframe

  if first: #Se periptwsh poy stis apanthseis den yparxei sxetiko keimeno gia kapoio erwthma tote to RR einai 0
    Reciprocal_Rank.append(0)

  total = sum(precision) #Synolo twn epimeroys precision (An to precision einai keno tote total 0)

  if count != 0: #An yphrxan sxetika keimena stis apanthseis tote vres total einai to Average_Precision gia to query alliws einai 0
    total /= count
  Average_Precision.append(total)

map = sum(Average_Precision)/len(Average_Precision) #Ypologismos MAP kai MRR
mrr = sum(Reciprocal_Rank)/len(Reciprocal_Rank)

Recall_Precision_df = (Recall_Precision_df/num_queries).round(4) #Sto dataframe me to synoliko recall-precision gia ola ta queries diairese me ton arithmo twn queries gia na vreis tis meses times
ax = Recall_Precision_df.plot(title="Recall-Precision diagram", xlabel="Recall percentage", ylabel="Precision percentage", yticks = [0,10,20,30,40,50,60,70,80,90,100]) #Ftiakse to diagramma recall-precision me ta stoixeia toy dataframe
ax.set_xticks(range(len(Recall_Precision_df)))
ax.set_xticklabels(["0%","10%","20%","30%","40%","50%","60%","70%","80%","90%","100%"])
ax.set_yticklabels(["0%","10%","20%","30%","40%","50%","60%","70%","80%","90%","100%"])

print()
plt.show() #Emfanise to diagramma recall-precision

print(f"\nThe MAP value over all 19 queries for k = {k} is equal to {map:.4f}") #Emfanish apotelesmatwn metrikwn MAP kai MRR
print(f"\nThe MRR value over all 19 queries for k = {k} is equal to {mrr:.4f}")